{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# we will use CUDA if it is available\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
    "print(\"CUDA:\", USE_CUDA)\n",
    "print(DEVICE)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    '''\n",
    "    a basic encoder-decoder architecture\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
    "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask, decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final, src_mask, trg_mask, hidden=decoder_hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    linear + softmax\n",
    "    '''\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        super(Generator,self).__init__()\n",
    "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \n",
    "        Return:\n",
    "        \n",
    "        output(the output of the hidden state at each time t) : (batch, seq_len, num_directions * hidden_size)\n",
    "        \n",
    "        final (final hidden state) : (num_layers, batch, num_directions * hidden_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        packed = pack_padded_sequence(x, lengths, batch_first = True)\n",
    "        output, final = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        #output : (batch, seq_len, num_directions * hidden_size)\n",
    "        #final :  (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        fwd_final = final[0:final.size(0):2] #[num_layers, batch, dim]\n",
    "        #from 0 to num_layers * num_directions with step 2, select forward final_hidden_state for every layer\n",
    "        \n",
    "        bwd_final = final[1:final.size(0):2] #[num_layers, batch, dim]\n",
    "        #from 1 to num_layers * num_directions with step 2, select backward final_hidden_state for every layer\n",
    "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
    "        \n",
    "        return output, final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # to initialize from the final encoder state\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
    "                                          hidden_size, bias=False)\n",
    "        \n",
    "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
    "        \"\"\"\n",
    "        Perform a single decoder step (1 word)\n",
    "        \n",
    "        Return : \n",
    "        pre_output : [batch_size,1,hidden_size]\n",
    "        \"\"\"\n",
    "\n",
    "        # compute context vector using attention mechanism\n",
    "        # hidden : last decoder hidden state\n",
    "        query = hidden[-1].unsqueeze(1)  # [num_layers, batch, hidden_size] -> [batch, 1, hidden_size]\n",
    "        context, attn_probs = self.attention(\n",
    "            query=query, proj_key=proj_key,\n",
    "            value=encoder_hidden, mask=src_mask)\n",
    "\n",
    "        # update rnn hidden state\n",
    "        #pre_embed : [batch_size,1,embed_size]\n",
    "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        \n",
    "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
    "        pre_output = self.dropout_layer(pre_output)\n",
    "        pre_output = self.pre_output_layer(pre_output)\n",
    "\n",
    "        return output, hidden, pre_output\n",
    "    \n",
    "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
    "                src_mask, trg_mask, hidden=None, max_len=None):\n",
    "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
    "                                         \n",
    "        # the maximum number of steps to unroll the RNN\n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        # initialize decoder hidden state\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_final)\n",
    "        \n",
    "        # pre-compute projected encoder hidden states\n",
    "        # (the \"keys\" for the attention mechanism)\n",
    "        # this is only done for efficiency\n",
    "        proj_key = self.attention.key_layer(encoder_hidden)\n",
    "        \n",
    "        # here we store all intermediate hidden states and pre-output vectors\n",
    "        decoder_states = []\n",
    "        pre_output_vectors = []\n",
    "        \n",
    "        # unroll the decoder RNN for max_len steps\n",
    "        for i in range(max_len):\n",
    "            #trg_embed : [batch_size, max_len]\n",
    "            prev_embed = trg_embed[:, i].unsqueeze(1) #pre_embed : [batch_size,1,embed_size]\n",
    "            output, hidden, pre_output = self.forward_step(\n",
    "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
    "            decoder_states.append(output)\n",
    "            pre_output_vectors.append(pre_output)\n",
    "\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
    "        return decoder_states, hidden, pre_output_vectors  #[batch_size, seq_len, hidden_size]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  # start with zeros\n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg, pad_index=0):\n",
    "        '''\n",
    "        src : [ batch_size, length-1 ] # 最开始什么都没有 最后应该是eos ??\n",
    "        src_mask : [batch, 1, length-1] #不等于PAD的地方是1 等于PAD是0\n",
    "        src_length : List(length of every batch(length-1)), len()=batch_size\n",
    "        \n",
    "        trg : [ batch_size, length-1 ] #最前面是 SOS\n",
    "        trg_mask : [batch, length-2] #不等于PAD的地方是1 等于PAD是0\n",
    "        trg_length : List(length of every batch(length)), len()=batch_size\n",
    "        \n",
    "        trg_y : [ batch_size, length-2 ] # NO EOS and SOS\n",
    "        ntokens : batch_size * length-2\n",
    "        nseqs : batch_size\n",
    "        \n",
    "        '''\n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src != pad_index ).unsqueeze(-2)\n",
    "        self.nseqs = src.size(0)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_mask = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "        \n",
    "        if trg is not None :\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg[:,:-1] # 去掉最后一列\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[:,1:]\n",
    "            self.trg_mask = (self.trg_y != pad_index)\n",
    "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
    "        \n",
    "        if USE_CUDA : \n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "            \n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                self.trg_mask = self.trg_mask.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, key_size=None, querry_size=None):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        \n",
    "        #a bi-directional encoder so key_size is 2*hidden_size \n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        querry_size = hidden_size if querry_size is None else querry_size\n",
    "        \n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.querry_layer = nn.Linear(querry_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "    \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\" \n",
    "        # mask 不能为空 \n",
    "        \n",
    "        # we first project the query (the decoder state)\n",
    "        # the projected keys (the encoder states) were already pre-computed\n",
    "        query = self.querry_layer(query)\n",
    "        \n",
    "        #calculate scores\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        \n",
    "        #0，1，2 把第二维是1去掉 在第一维加1\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # mask out invalid positions\n",
    "        # the mask marks valid positions so we invert it using 'mask & 0'\n",
    "        scores.data.masked_fill_(mask==0, -float('inf'))\n",
    "        \n",
    "        #turn scores to probabilities.\n",
    "        alphas = F.softmax(scores, dim=-1)\n",
    "        self.alphas = alphas\n",
    "        \n",
    "        context = torch.bmm(alphas, value)\n",
    "        \n",
    "        #context shape : [Batch_size, 1, 2*hidden_size]\n",
    "        #alphas shape : [Batch_size, 1, seq_len]\n",
    "        return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
    "    attention = BahdanauAttention(hidden_size)\n",
    "    \n",
    "    model = EncoderDecoder(\n",
    "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
    "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
    "        nn.Embedding(src_vocab, emb_size),\n",
    "        nn.Embedding(tgt_vocab, emb_size),\n",
    "        Generator(hidden_size,tgt_vocab)\n",
    "    )\n",
    "    \n",
    "    return model.cuda() if USE_CUDA else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    print_tokens = 0\n",
    "    \n",
    "    for i, batch in enumerate(data_iter, 1):\n",
    "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
    "                                           batch.src_mask, batch.trg_mask,\n",
    "                                           batch.src_lengths, batch.trg_lengths)\n",
    "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        print_tokens += batch.ntokens\n",
    "        \n",
    "        if model.training and i % print_every == 0 :\n",
    "            elapes = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.nseqs, print_tokens / elapes))\n",
    "            start = time.time()\n",
    "            print_tokens = 0 \n",
    "        \n",
    "    return math.exp(total_loss/float(total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1,x.size(-1)),\n",
    "                             y.contiguous().view(-1))\n",
    "        \n",
    "        loss = loss/norm\n",
    "        \n",
    "        if self.opt is not None:\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "            \n",
    "        return loss.data.item() * norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
    "        prev_y = torch.ones(1,1).fill_(sos_index).type_as(src)\n",
    "        trg_mask = torch.ones_like(prev_y)\n",
    "    \n",
    "    output = []\n",
    "    attention_scores = []\n",
    "    hidden = None\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out, hidden, pre_output = model.decode(encoder_hidden, encoder_final, src_mask, prev_y, trg_mask, hidden)\n",
    "            \n",
    "            # we predict from the pre-output layer, which is a combination of decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:,-1])\n",
    "            \n",
    "        _, next_word = torch.max(prob,dim =1)\n",
    "        next_word = next_word.data.item()\n",
    "        output.append(next_word)\n",
    "        prev_y = torch.ones(1,1).type_as(src).fill_(next_word)\n",
    "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
    "        \n",
    "    output = np.array(output)\n",
    "    \n",
    "    # cut off everything statring from </s>\n",
    "    # only when eos_index provided\n",
    "    if eos_index is not None :\n",
    "        first_eos = np.where(output ==eos_index)[0]\n",
    "        if len(first_eos) > 0:\n",
    "            output = output[:first_eos[0]]\n",
    "    return output, np.concatenate(attention_scores, axis =1)\n",
    "\n",
    "def lookup_words(x, vocab=None):\n",
    "    if vocab is not None:\n",
    "        # itos 把一个整数转换为字符串\n",
    "        x = [vocab.itos[i] for i in x]\n",
    "    return [str(t) for t in x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "def beamsearch_decode():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(example_iter, model, n=2, max_len=100, sos_index=1, \n",
    "                   src_eos_index=None, trg_eos_index=None,\n",
    "                  src_vocab=None, trg_vocab=None):\n",
    "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    print()\n",
    "    \n",
    "    if src_vocab is not None and trg_vocab is not None :\n",
    "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
    "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
    "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
    "    else:\n",
    "        src_eos_index = None\n",
    "        trg_sos_index = 1\n",
    "        trg_eos_index = None\n",
    "    \n",
    "    for i, batch in enumerate(example_iter):\n",
    "        src = batch.src.cpu().numpy()[0,:]\n",
    "        trg = batch.trg_y.cpu().numpy()[0,:]\n",
    "        \n",
    "        #remove </s> (if it is there)\n",
    "        src = src[:-1] if src[-1] == src_eos_index else src\n",
    "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg\n",
    "        \n",
    "        result, _ = greedy_decode(\n",
    "            model, batch.src, batch.src_mask, batch.src_lengths,\n",
    "            max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index\n",
    "        )\n",
    "        \n",
    "        print(\"Example #%d\" % (i+1))\n",
    "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
    "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
    "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
    "        print()\n",
    "        \n",
    "        count += 1\n",
    "        if count == n:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"    \n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "LOWER = True\n",
    "MAX_LEN = 25\n",
    "MIN_FREQ = 5\n",
    "if True :\n",
    "    \n",
    "    spacy_de = spacy.load('de')\n",
    "    spacy_en = spacy.load('en')\n",
    "    \n",
    "    def tokenize_de(text):\n",
    "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "    \n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    SRC = data.Field(tokenize=tokenize_de, \n",
    "                     batch_first=True, lower=LOWER,include_lengths=True,\n",
    "                    unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN,eos_token=EOS_TOKEN)\n",
    "    \n",
    "    TRG = data.Field(tokenize=tokenize_en, \n",
    "                     batch_first=True, lower=LOWER, include_lengths=True,\n",
    "                     unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "    \n",
    "    train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
    "        exts=('.de', '.en'), fields=(SRC, TRG), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)\n",
    "    \n",
    "    SRC.build_vocab(train_data.src, min_freq = MIN_FREQ)\n",
    "    TRG.build_vocab(train_data.trg, min_freq = MIN_FREQ)\n",
    "    \n",
    "    PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 143116\n",
      "valid 690\n",
      "test 963 \n",
      "\n",
      "First training example:\n",
      "src: david gallo : das ist bill lange . ich bin dave gallo .\n",
      "trg: david gallo : this is bill lange . i 'm dave gallo . \n",
      "\n",
      "Most common words (src):\n",
      "         .     138325\n",
      "         ,     105944\n",
      "       und      41839\n",
      "       die      40809\n",
      "       das      33324\n",
      "       sie      33035\n",
      "       ich      31153\n",
      "       ist      31035\n",
      "        es      27449\n",
      "       wir      25817 \n",
      "\n",
      "Most common words (trg):\n",
      "         .     137259\n",
      "         ,      91619\n",
      "       the      73344\n",
      "       and      50273\n",
      "        to      42798\n",
      "         a      39573\n",
      "        of      39496\n",
      "         i      33524\n",
      "        it      32921\n",
      "      that      32643 \n",
      "\n",
      "First 10 words (src):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 .\n",
      "05 ,\n",
      "06 und\n",
      "07 die\n",
      "08 das\n",
      "09 sie \n",
      "\n",
      "First 10 words (trg):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 .\n",
      "05 ,\n",
      "06 the\n",
      "07 and\n",
      "08 to\n",
      "09 a \n",
      "\n",
      "Number of German words (types): 15762\n",
      "Number of English words (types): 13003 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
    "\n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\n",
    "    \n",
    "    \n",
    "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(train_data, batch_size=128, train=True,\n",
    "                                sort_within_batch=True,\n",
    "                                sort_key=lambda x :(len(x.src), len(x.trg)), repeat=False,\n",
    "                                device=DEVICE)\n",
    "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False, device=DEVICE)\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    return Batch(batch.src, batch.trg, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=1, lr=0.0003, print_every=100):\n",
    "    \"\"\"Train a model on IWSLT\"\"\"\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    # optionally add label smoothing; see the Annotated Transformer\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    dev_perplexities = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      \n",
    "        print(\"Epoch\", epoch)\n",
    "        model.train()\n",
    "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
    "                                     model,\n",
    "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
    "                                     print_every=print_every)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
    "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
    "\n",
    "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
    "                                       model, \n",
    "                                       SimpleLossCompute(model.generator, criterion, None))\n",
    "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
    "            dev_perplexities.append(dev_perplexity)\n",
    "        \n",
    "    return dev_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cuihongyi19941128/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 100 Loss: 129.366684 Tokens per Sec: 13520.394376\n",
      "Epoch Step: 200 Loss: 40.538223 Tokens per Sec: 15277.545507\n",
      "Epoch Step: 300 Loss: 114.885857 Tokens per Sec: 15580.349217\n",
      "Epoch Step: 400 Loss: 34.839153 Tokens per Sec: 14683.912734\n",
      "Epoch Step: 500 Loss: 38.983398 Tokens per Sec: 13805.155045\n",
      "Epoch Step: 600 Loss: 31.550386 Tokens per Sec: 14297.136099\n",
      "Epoch Step: 700 Loss: 63.275188 Tokens per Sec: 15463.767043\n",
      "Epoch Step: 800 Loss: 53.533489 Tokens per Sec: 15243.236547\n",
      "Epoch Step: 900 Loss: 47.404831 Tokens per Sec: 14146.720483\n",
      "Epoch Step: 1000 Loss: 105.892426 Tokens per Sec: 13681.503347\n",
      "Epoch Step: 1100 Loss: 72.203178 Tokens per Sec: 14495.406366\n",
      "\n",
      "Example #1\n",
      "Src :  <s> als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
      "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "Pred:  when i was a few years ago , i was a <unk> of the <unk> of the <unk> .\n",
      "\n",
      "Example #2\n",
      "Src :  <s> mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
      "Trg :  my father was listening to bbc news on his small , gray radio .\n",
      "Pred:  my mother was in a little bit , the <unk> <unk> <unk> the <unk> of the <unk> .\n",
      "\n",
      "Example #3\n",
      "Src :  <s> er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
      "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "Pred:  he 's very much more important , but there was a <unk> of the <unk> .\n",
      "\n",
      "Validation perplexity: 41.457666\n"
     ]
    }
   ],
   "source": [
    "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
    "                   emb_size=256, hidden_size=256,\n",
    "                   num_layers=1, dropout=0.2)\n",
    "dev_perplexities = train(model, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a2aa54390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_perplexity(perplexities):\n",
    "    \"\"\"plot perplexities\"\"\"\n",
    "    plt.title(\"Perplexity per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.plot(perplexities)\n",
    "    \n",
    "plot_perplexity(dev_perplexities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
